{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "My Hierarchical Reinforcement Learning architecture consists of a lower-level (Actions) PPO policy which chooses actions for the environment and a higher-level (Goaly) PPO policy which sets goals for the Actions policy.  There is also an inverse-dynamics model trained to predict both the action and goal taken at each step.  \n",
    "\n",
    "<img src=\"goaly_model.png\" alt=\"\" width=\"600\"/>\n",
    "\n",
    "\n",
    "## Intrinsic Reward\n",
    "\n",
    "Both policies are trained separately based on extrinsic and intrinsic rewards.  The extrinsic reward is just the reward from the environment, while the intrinsic reward is calclulated based on the predictions made by the inverse dynamics model.  Inverse dynamics model is a modeled by a neural network which predicts actions and goals.  Training this neural network is done by learning function *m* defined as the following:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{a}, \\hat{g} = m(s_t, s_t+t; \\phi_I)\n",
    "\\end{equation*}\n",
    "\n",
    "where, $\\hat{a}_t$ is the predicted estimate of the action $a_t$, and $\\hat{g}_t$ is the predicted estimate of the goal $g_t$.  The neural network parameters $\\phi_I$ are trained to optimize \n",
    "\n",
    "\\begin{equation*}\n",
    "\\underset{\\phi_I}{min} = L_I(\\hat{a}_t, \\hat{g}_t, a_t, g_t)\n",
    "\\end{equation*}\n",
    "\n",
    "where $L_I$ is the loss function that measures the difference between the predicted actions and goals, and actions and goals observed during agent's interaction with the environment.\n",
    "\n",
    "### Stability\n",
    "The main purpose of the intrinsic reward is to keep the interpretation of goals by the Actions policy stable.  The second purpose is to encourage assignment of new goals when exploring new areas of the state-space.  The stability reward uses two values computed by the inverse model *m*:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\bar{a} = \\frac{\\hat{a}_t - a_t}{\\hat{a}_t + a_t}\n",
    "\\end{equation*}\n",
    ", where the denominator ensures that the reward doesn't favor low-amplitude actions and is capped to avoid divides by zero, and keep the difference value normalized near *0,1* range.  \n",
    "\n",
    "\\begin{equation*}\n",
    "\\bar{g} = \\frac{\\hat{g}_t - g_t}{g_{count}}\n",
    "\\end{equation*}\n",
    ", where goals are represented by a scalar (not one hot).  This has the effect of penalizing small goal differences less than large ones, which means that goal space is treated as a continuum rather than as completely independent values.\n",
    "\n",
    "Finally the stability reward is defined as\n",
    "\n",
    "\\begin{equation*}\n",
    "S = 1 + \\bar{g}(\\bar{a} - 1) + \\bar{a}(\\bar{g} - 1)\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Exploration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spinning Up",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
