{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Agent architecture\n",
    "\n",
    "### PPOBuffer\n",
    "* PPO\n",
    " * ``obs_buf (flaot)(size, obs_dim)``: Environment observations.\n",
    " * ``act_buf (float)(size, act_dim)``: Actions taken in environment\n",
    " * ``adv_buf (float) (size)`` \n",
    "  * ``deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]``\n",
    "  * ``self.adv_buf[path_slice] = core.discount_cumsum(deltas, self.gamma * self.lam)``\n",
    " * ``rew_buf (float) (size)``: rewards received at step\n",
    " * ``ret_buf (float) (size)``: rewards-to-go. Rewards to be accumulated following this step, used to train the \n",
    "   value function.\n",
    "  * ``self.ret_buf[path_slice] = core.discount_cumsum(rews, self.gamma)[:-1]``\n",
    " * ``val_buf (float) (size)``: value function estimate at step\n",
    " * ``logp_buf (float) (size)``: log probabilities of action at step\n",
    " * ``gamma(float)``: Discount factor of future rewards. (Always between 0 and 1.) \n",
    " * ``lam (float)``: Lambda for GAE-Lambda. (Always between 0 and 1, close to 1.)\n",
    " * ``path_start_idx``: index where current trajectory through environment started.  \n",
    "  * This looks back in the buffer to where the trajectory started, and uses rewards and value estimates from\n",
    "    the whole trajectory to compute advantage estimates with GAE-Lambda, as well as compute the \n",
    "    rewards-to-go for each state, to use as the targets for the value function.\n",
    " * ``ptr``: current index into buffer \n",
    " * ``self.max_size``: buffer size limit\n",
    "* Inverse Model\n",
    " * ``goal_buf(flaot) (size, goals_dim)``\n",
    "\n",
    "## actor_critic\n",
    "* ``pi``       (batch, act_dim) Samples actions from policy given states.\n",
    "*  ``logp``     (batch,) Gives log probability, according to the policy, of taking actions ``a_ph``\n",
    "in states ``x_ph``.\n",
    "* ``logp_pi``  (batch,) Gives log probability, according to the policy, of the action \n",
    "sampled by ``pi``.\n",
    "* ``v``        (batch,) Gives the value estimate in ``x_ph``. (Critical: make sure\n",
    "to flatten this!)\n",
    "\n",
    "\n",
    "## Inverse Model\n",
    "* inputs: \n",
    " * ``obs (float) (2, obs_dim)``, flatten\n",
    "* predict: \n",
    " * ``action_predicted (float) (act_dim)``\n",
    " * ``action_probability = exp(logp_pi)``\n",
    " * ``goal_predicted (float) (goals_dim)``\n",
    "* loss: \n",
    " * ``action_error = (action - action_predicted)``\n",
    " * ``inverse_model_goal_error = (goal - goal_predicted) * (action_error * action_probability + goal_error_base)``\n",
    " * ``inverse_model_goal_error_base``: parameter (0, 1) that defines the minimum correction for goals, even when action prediction is very accurate (action_error ~= 0)\n",
    " * ``inverse_model_loss = mean_squared_error(action_error) + mean_squared_error(goal_error)``\n",
    "\n",
    "\n",
    "## Action Policy Model\n",
    "\n",
    "* action policy reward: The action selection policy is rewarded for stability.  This means that the action policy is rewarded for selecting actions consistently in context of repeated observations.  Action policy is parametrized by goals and it is desired that different goals produce different trajectories through the environment.  The stability reward penalizes well-known actions (based on action error), if they are taken in context of the wrong goal.  This should steer the action policy to chose new actions in context of new goals.\n",
    " * ``reward_goal_error = (goal - goal_predicted)``\n",
    " * ``reward_action_error = (action - action_predicted) * action_probability``\n",
    " * ``action_error_discount``: How much action error should reduce stability bonus. If there is a large\n",
    "   error in predicting the action in the inverse model, it indicates that the action policy is exploring \n",
    "   a new region of the environment.  In this case, guessing the wrong goal should not pentalize the policy.\n",
    " * ``action_error_factor``: How much the action error contributes to the stability loss.  \n",
    " * ``goal_error_factor``: How much the goal error contributes to the stability loss.  \n",
    " * ``stability_loss = goal_error_factor * reward_goal_error * (1 - reward_action_error) + action_error_factor * reward_action_error * (1 - reward_goal_error)\n",
    "\n",
    "\n",
    "## Goal Policy Reward\n",
    "* goal policy: Selects goals that are given to the Action Policy as input.  The goal policy is a PPO policy that maximizes long-term rewards.  The rewards are both external and internal, where the external reward is given by the enrivonment, and the internal reward is the same stability reward used for the action policy.  Both rewards are attenuated over time through habituation.  The internal reward is habituated by a factor that is separate for each goal. If a given goal is selected, it's factor is decreased by ``stability_habituation_factor``, and increased by ``stability_restoration_factor`` when the goal is not selected.  The external rewards are similarly controlled by ``reward_habituation_factor`` and ``reward_restoration_factor``, which are used depending on whether the reward is received on a given step.  If the reward is above mean, then reward is habituated, otherwise restored.\n",
    " * ``stability_habituation`` (float)(num_goals): vector of current habituation factors.  Initially set to 1 for each goal.\n",
    " * ``reward_habituation`` (float)(num_rewards): scalar indicating current reward habituation, initially set to 1.\n",
    "\n",
    "## Inverse Dynamics\n",
    "\n",
    "## Lower Level Policy\n",
    "\n",
    "## Higher Level Policy\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
